{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8000303,"sourceType":"datasetVersion","datasetId":4711105}],"dockerImageVersionId":30673,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import string\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\npd.options.mode.chained_assignment = None\n\nimport nltk\nfrom nltk.sentiment import SentimentIntensityAnalyzer\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem import WordNetLemmatizer\n!unzip /usr/share/nltk_data/corpora/wordnet.zip -d /usr/share/nltk_data/corpora/\n\nfrom wordcloud import WordCloud, STOPWORDS\n\nfrom transformers import pipeline\n\ndf = pd.read_csv(\"/kaggle/input/simplygo-results/results.csv\")\n\nprint(df.info(), \"\\n\")\ndf","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"nullDf = df[df.isna().any(axis=1)]\nnullDf","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df.dropna()\ndf['Date'] = pd.to_datetime(df['Date'], format='%b-%y') \ndf.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stop_words = stopwords.words('english')\n\ndef clean_text(text):\n    text = text.lower()\n    text = text.replace(\"\\\\\", \"\")\n    text = text.translate(str.maketrans('', '', string.punctuation))\n    \n    tokens = word_tokenize(text)\n    \n    words = [word for word in tokens if word not in stop_words]\n    \n    lemmatizer = WordNetLemmatizer()\n    lemmatized_words = [lemmatizer.lemmatize(word) for word in tokens]\n    \n    return ' '.join(lemmatized_words)\n\ndf['Article Content'] = df['Article Content'].apply(clean_text)\ndf","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sia = SentimentIntensityAnalyzer()\nresults = {}\n\nfor index, row in df.iterrows():\n    text = row[\"Article Content\"]\n    row_id = index\n    results[row_id] = sia.polarity_scores(text)\n\nsentiment_score_df = pd.DataFrame(results).T\nsentiment_score_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df.merge(sentiment_score_df, on=df.index)\ndf","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = df.drop(['key_0'], axis=1)\ndf","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.to_csv('my_data.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"positive_df = df[df[\"compound\"] >= 0].reset_index().drop(['index'], axis=1)\nnegative_df = df[df[\"compound\"] < 0].reset_index().drop(['index'], axis=1)\n\nprint(\"Positive Mean: \", positive_df[\"compound\"].mean())\nprint(\"Negative Mean: \", negative_df[\"compound\"].mean())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"negative_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for url in negative_df[\"URL\"]:\n    print(url)\n    \nprint(\"\\n\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for title in negative_df[\"Title\"]:\n    print(title)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"remove_words = [\"mp\", \"attack\", \"teenager\", \"south africa\", \"international\", \"drug\", \"supremacist\", \"newsletter\", \"white\", \"pm\", \"mar\", \"s270\", \"u\", \"hong\", \"fourth\", \"ha\", \"wa\", \"one\", \"two\", \"t\", \",\", '\"', \"sg\", \"ll\", \"ng\", \"st\", \"chee\", \"s\", \"war\", \"fu\", \"prison\", \"jailed\", \"criminal\", \"murder\", \"violent\", \"bomb\", \"shit\", \"killed\", \"molested\", \"terrorist\", \"arrested\", \"brutally\", \"racism\", \"negative\", \"death\", \"perjury\", \"trauma\"]\nfig, axes = plt.subplots(nrows=3, ncols=4, figsize=(20, 10))\n\nfor index, (text, ax) in enumerate(zip(negative_df[\"Article Content\"], axes.flatten())):\n    word_list = []\n    tokens = word_tokenize(text)\n\n    for word in tokens:\n        score = sia.polarity_scores(word)['compound']\n        if score <= -0.3:\n                word_list.append(word)\n    \n    words = ' '.join(word_list)\n    \n    wc_stopwords = list(STOPWORDS) + remove_words\n    wc_params = {\n        'background_color': 'white',\n        'width': 170,\n        'height': 170,\n        'stopwords': wc_stopwords,\n        'colormap': 'tab10',\n        'max_words': 35\n    }\n    \n    wordcloud = WordCloud(**wc_params).generate(text)\n    ax.imshow(wordcloud, interpolation='bilinear')\n    ax.axis('off')\n    \n    title = negative_df[\"Title\"][index]\n    split_title = title.split()\n            \n    wrapped_title = '\\n'.join([' '.join(split_title[i:i +5]) for i in range(0, len(words), 6)]).rstrip()\n    \n    ax.set_title(f\"{index+1}: {wrapped_title}\")\n    ax.title.set_fontsize(9) \n\nplt.savefig('wordclouds.png')\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}